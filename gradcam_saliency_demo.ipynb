{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afbb1b01",
   "metadata": {},
   "source": [
    "# Saliency Maps Demo (Grad-CAM)\n",
    "\n",
    "This notebook generates a **Grad-CAM** saliency map for an image classification model (**ResNet-18** pretrained on ImageNet).\n",
    "\n",
    "## What you’ll learn\n",
    "- How to compute a Grad-CAM heatmap for a CNN prediction\n",
    "- How to visualize when a model may be focusing on the **background** instead of the object\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "You need: `torch`, `torchvision`, `Pillow`, `matplotlib`\n",
    "\n",
    "If you run into installation issues, use a standard PyTorch + torchvision environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab9aad3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m models, transforms\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7313da2",
   "metadata": {},
   "source": [
    "## 1) Get the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f5db1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://zenodo.org/records/15376499/files/demo_data_clf.zip?download=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416c310c",
   "metadata": {},
   "source": [
    "Get an image\n",
    "\n",
    "Place an image file named **`animal.jpg`** in the same folder as this notebook.\n",
    "- Good choices: wildlife photo, camera-trap image, landscape with an animal, etc.\n",
    "- The demo is more interesting when the **background is dominant**, because the model may latch onto it.\n",
    "\n",
    "If you prefer a different filename, just edit the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8398173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image (edit filename as needed)\n",
    "image_path = \"animal.jpg\"\n",
    "\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e891db3",
   "metadata": {},
   "source": [
    "## 2) Load a pretrained model + define preprocessing\n",
    "\n",
    "We use **ResNet-18** pretrained on ImageNet.  \n",
    "Preprocessing must match the training normalization used for ImageNet models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186f8775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# ImageNet preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "x = transform(img).unsqueeze(0)  # shape: (1, 3, 224, 224)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfdf699",
   "metadata": {},
   "source": [
    "## 3) Grad-CAM implementation\n",
    "\n",
    "**Idea:** take gradients of the predicted class score w.r.t. a convolutional feature map, then produce a weighted sum of channels to obtain a class-discriminative heatmap.\n",
    "\n",
    "We’ll hook into the **last convolutional block** (`layer4`) for a useful spatial map.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1419dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.activations = None\n",
    "        self.gradients = None\n",
    "\n",
    "        # Forward hook: store activations\n",
    "        self.fwd_handle = self.target_layer.register_forward_hook(self._forward_hook)\n",
    "\n",
    "        # Backward hook: store gradients (use full backward hook when available)\n",
    "        if hasattr(self.target_layer, \"register_full_backward_hook\"):\n",
    "            self.bwd_handle = self.target_layer.register_full_backward_hook(self._backward_hook)\n",
    "        else:\n",
    "            # Fallback for older PyTorch\n",
    "            self.bwd_handle = self.target_layer.register_backward_hook(self._backward_hook)\n",
    "\n",
    "    def _forward_hook(self, module, inputs, output):\n",
    "        self.activations = output  # shape: (N, C, H, W)\n",
    "\n",
    "    def _backward_hook(self, module, grad_input, grad_output):\n",
    "        # grad_output[0] corresponds to gradient w.r.t. the layer output\n",
    "        self.gradients = grad_output[0]  # shape: (N, C, H, W)\n",
    "\n",
    "    def __call__(self, x, class_idx=None):\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        logits = self.model(x)\n",
    "        if class_idx is None:\n",
    "            class_idx = int(logits.argmax(dim=1).item())\n",
    "\n",
    "        # Backward: gradient of the selected logit\n",
    "        score = logits[:, class_idx].sum()\n",
    "        score.backward(retain_graph=True)\n",
    "\n",
    "        # Compute weights: global-average-pool gradients over spatial dims\n",
    "        weights = self.gradients.mean(dim=(2, 3), keepdim=True)  # (N, C, 1, 1)\n",
    "\n",
    "        # Weighted sum of activations\n",
    "        cam = (weights * self.activations).sum(dim=1)  # (N, H, W)\n",
    "        cam = F.relu(cam)  # keep only positive influence\n",
    "\n",
    "        # Normalize to [0, 1]\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / (cam.max() + 1e-8)\n",
    "\n",
    "        return cam.detach(), logits.detach(), class_idx\n",
    "\n",
    "    def close(self):\n",
    "        self.fwd_handle.remove()\n",
    "        self.bwd_handle.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f4146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a target layer: last conv block in ResNet-18 is model.layer4\n",
    "target_layer = model.layer4\n",
    "gradcam = GradCAM(model, target_layer)\n",
    "\n",
    "cam, logits, pred_class = gradcam(x)\n",
    "pred_class, logits.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6b259f",
   "metadata": {},
   "source": [
    "## 4) Visualize the Grad-CAM heatmap overlay\n",
    "\n",
    "This overlay helps you see **where the model is focusing** for its prediction.\n",
    "\n",
    "> Teaching angle: if the heatmap mostly lights up the **background** rather than the animal/object, the model may be “right for the wrong reasons.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c3ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: overlay heatmap on original image\n",
    "def overlay_cam_on_image(pil_img, cam_2d, alpha=0.5):\n",
    "    # cam_2d: torch tensor (H, W) in [0, 1] at 224x224 resolution\n",
    "    cam_np = cam_2d.cpu().numpy()\n",
    "    cam_np = np.uint8(255 * cam_np)\n",
    "\n",
    "    # Resize cam to original image size\n",
    "    cam_img = Image.fromarray(cam_np).resize(pil_img.size, resample=Image.BILINEAR)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(pil_img)\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(pil_img)\n",
    "    plt.imshow(cam_img, cmap=\"jet\", alpha=alpha)\n",
    "    plt.title(\"Grad-CAM Overlay\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "overlay_cam_on_image(img, cam[0], alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e277d04",
   "metadata": {},
   "source": [
    "## 5) Show “incorrect attention” with a quick occlusion test (optional but powerful)\n",
    "\n",
    "A simple sanity check:\n",
    "\n",
    "- Mask out the **high-saliency** region → prediction *should drop* if that region truly matters.\n",
    "- Mask out the **low-saliency** region → prediction *should change less*.\n",
    "\n",
    "This often reveals when the model depends on background cues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b205e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: occlusion sanity check\n",
    "def occlude_by_cam(x, cam_2d, keep_high=True, frac=0.25):\n",
    "    '''\n",
    "    Occlude either the top-salient region (keep_high=False) or\n",
    "    the low-saliency region (keep_high=True) based on a quantile threshold.\n",
    "    '''\n",
    "    x2 = x.clone()\n",
    "    cam_np = cam_2d.cpu().numpy()\n",
    "    thresh = np.quantile(cam_np, 1 - frac)  # top frac\n",
    "    mask = (cam_np >= thresh)\n",
    "\n",
    "    if keep_high:\n",
    "        # Keep high-saliency; occlude the rest\n",
    "        occ = ~mask\n",
    "    else:\n",
    "        # Occlude high-saliency\n",
    "        occ = mask\n",
    "\n",
    "    # Expand to channels\n",
    "    occ_t = torch.from_numpy(occ).to(x.device)\n",
    "    occ_t = occ_t.unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
    "    occ_t = occ_t.expand(-1, 3, -1, -1)      # (1,3,H,W)\n",
    "\n",
    "    # Zero out occluded pixels (in normalized space)\n",
    "    x2[occ_t] = 0.0\n",
    "    return x2\n",
    "\n",
    "def softmax_prob(logits, idx):\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    return float(probs[0, idx].item())\n",
    "\n",
    "# Baseline prediction\n",
    "with torch.no_grad():\n",
    "    base_logits = model(x)\n",
    "base_prob = softmax_prob(base_logits, pred_class)\n",
    "\n",
    "# Occlude high-saliency region\n",
    "x_occ_high = occlude_by_cam(x, cam[0], keep_high=False, frac=0.25)\n",
    "with torch.no_grad():\n",
    "    logits_occ_high = model(x_occ_high)\n",
    "prob_occ_high = softmax_prob(logits_occ_high, pred_class)\n",
    "\n",
    "# Occlude low-saliency region (keep only high-saliency)\n",
    "x_keep_high = occlude_by_cam(x, cam[0], keep_high=True, frac=0.25)\n",
    "with torch.no_grad():\n",
    "    logits_keep_high = model(x_keep_high)\n",
    "prob_keep_high = softmax_prob(logits_keep_high, pred_class)\n",
    "\n",
    "print(f\"Predicted class index: {pred_class}\")\n",
    "print(f\"Baseline prob (pred class):            {base_prob:.4f}\")\n",
    "print(f\"Prob after occluding HIGH-saliency:    {prob_occ_high:.4f}\")\n",
    "print(f\"Prob after keeping only HIGH-saliency: {prob_keep_high:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a5ebeb",
   "metadata": {},
   "source": [
    "### Visualize the occluded images (optional)\n",
    "\n",
    "These images are in **normalized tensor space**, so they won’t look perfectly natural, but they’re useful for inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df2d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize occluded versions (de-normalize for display)\n",
    "def denorm(t):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)\n",
    "    t = t * std + mean\n",
    "    t = torch.clamp(t, 0, 1)\n",
    "    return t\n",
    "\n",
    "def show_tensor_image(t, title):\n",
    "    t = denorm(t).squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(t)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "show_tensor_image(x, \"Input (de-normalized)\")\n",
    "show_tensor_image(x_occ_high, \"Occlude HIGH-saliency region\")\n",
    "show_tensor_image(x_keep_high, \"Keep only HIGH-saliency region\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6229846",
   "metadata": {},
   "source": [
    "## Interpretation notes (for your lecture)\n",
    "\n",
    "- If occluding the “important” region **doesn’t** reduce the model’s confidence much,\n",
    "  the saliency may be misleading or the model may be relying on broad context.\n",
    "- If the highlighted region is mostly **background**, the model may not be learning the object.\n",
    "\n",
    "**Key takeaway:** interpretability tools help you spot *how a model might fail*, especially under domain shift.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv4e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
